{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rqs\n",
    "import time\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import threading \n",
    "cookies={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После некоторого раздумия, сервис **[hidemy.name](https://hidemy.name/ru/proxy-list/)** показался мне предпочтительнее **[proxyrotator.com](https://www.proxyrotator.com/free-proxy-list/)**. Если на втором можно только получить список прокси, а потом отбирать нужные, то на первом можно задавать множество параметров поиска. Что приятно. Если уж быть абсолютно честным до конца, именно первый я по началу и хотел использовать. Но испугала процедура проверки веб-клиента на \"человечность\". Фильтрация невидимых элементов и распознавание чисел на картинках (**[proxyrotator.com](https://www.proxyrotator.com/free-proxy-list/)**) показалось занятием менее сложным.   \n",
    "\n",
    "Если Вы удалите куки, связанные с **hidemy.name** и перейдёте по соответствующей **[ссылке](https://hidemy.name/ru/proxy-list/)**, сначала появится страничка, проверяющая браузер на \"человечность\" и только потом загрузится список прокси.  \n",
    "\n",
    "Давайте попробуем прочитать страничку с **[hidemy.name](https://hidemy.name/ru/proxy-list/)** наиболее тупым способом, самой низкоуровневой питоньей библиотекой **urllib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urlrq\n",
    "import urllib.error as err\n",
    "\n",
    "url=\"https://hidemy.name/ru/proxy-list/\"\n",
    "try:\n",
    "    urlrq.urlopen(url)\n",
    "except err.URLError as e :\n",
    "    print(e.code)\n",
    "    s=str(e.read().decode(\"utf-8\"))\n",
    "    file=open(\"hideme_403.html\", \"wt\")\n",
    "    file.write(s)\n",
    "    file.close()\n",
    "\n",
    "#Попробуйте здесь закомментировать или наоборот раскомментировать некоторые заголовки    \n",
    "rq=urlrq.Request(url)\n",
    "rq.add_header(\"User-Agent\", \"Mozilla/5.0 (X11; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0\")\n",
    "rq.add_header(\"Accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\")\n",
    "#rq.add_header(\"Accept-Encoding\", \"gzip, deflate\")\n",
    "#rq.add_header(\"Accept-Language\", \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\")\n",
    "#rq.add_header(\"Connection\", \"keep-alive\")\n",
    "try:\n",
    "    urlrq.urlopen(rq)\n",
    "except err.URLError as e :\n",
    "    print(e.code)\n",
    "    s=str(e.read().decode(\"utf-8\"))\n",
    "    file=open(\"hideme_503.html\", \"wt\")\n",
    "    file.write(s)\n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом случае получили 403. Откройте сохранённый файл, и Вы не увидите ничего похожего на проверочную страничку !    \n",
    "\n",
    "Во втором (если Вы конечно не закомментировали необходимые заголовки), получаем 503, а сохранённый файл выглядит так, как он и должен выглядеть. \n",
    "\n",
    "Этот пример чисто учебный. На самом деле мы будем пользоваться более высокоуровневой библиотекой **requests**, которая судя по всему добавляет какие-то более не менее вменяемые заголовки по умолчанию (не проверял этот вопрос) и всегда получает 503. Но наш первый пример показывает, насколько важны тут заголовки. И намекает на то, что неплохо бы не надеяться на умолчательные, а собирать их ручками. Давайте так и сделаем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Это я просто выдрал из своей огненной лисички\n",
    "firefox={\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",    \n",
    "\"Accept-Encoding\": \"gzip, deflate\", \n",
    "\"Accept-Language\": \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"Host\": \"hidemy.name\",\n",
    "\"Referer\": \"https://hidemy.name/en/\",        \n",
    "\"Upgrade-Insecure-Requests\": \"1\",    \n",
    "\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0\"   \n",
    "}\n",
    "\n",
    "def hidemy():\n",
    "    return rqs.get(\"https://hidemy.name/en/proxy-list/\", headers=firefox)\n",
    "\n",
    "rsp=hidemy()\n",
    "s=str(rsp.content.decode(rsp.encoding))\n",
    "file=open(\"hideme_start.html\", \"wt\")\n",
    "file.write(s)\n",
    "file.close()\n",
    "bsObj=soup(s, \"lxml\")\n",
    "print(rsp.status_code)\n",
    "rsp.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили 503. В заголовке пришла кука с именем **__cfduid**, а сохранённый файл содержит немудрящий тест. Нужно подождать четыре секунды, потом выполнить несложный арифметический пример, забить его в форму и отправить эту форму на сервер. Всё бы ничего, но числа для этого примера записаны немного странным образом. Например вот это \"число\" - **+((!+[]+!![]+!![]+!![]+[])+(!+[]+!![]+!![]))** на самом деле равно **43**. Спасибо **j0hnik**, который помог с этим разобраться. Прежде всего давайте соберём все эти \"числа\". Посмотрев на файл видим, что сначала \"число\" присваивается некоторой переменной. А затем эта переменная участвует в операциях **+=**, **-=** и ***=**.  \n",
    "В строке где переменной присваивается значение, много запятых, что позволяет её легко найти. Сделаем это: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNums(bsObj):\n",
    "    res=[]                                         #результат\n",
    "    script=str(bsObj.find(\"script\").contents[0])   #находим javascript\n",
    "    a=script.split(\"\\n\")                           #разбиваем по символам новой строки\n",
    "    mx=0                                 #максимум числа запятых в строке\n",
    "    idx=0                                #местоположение максимума\n",
    "    for i in range(0, len(a)):      #ищем строку с максимальным количеством запятых\n",
    "        aa=a[i].split(\",\")\n",
    "        if len(aa)>mx:\n",
    "            mx=len(aa)\n",
    "            idx=i\n",
    "    s=a[idx]                        #берём её часть справа от последней запятой\n",
    "    s=s[s.rfind(\",\")+1:].strip()    #и удаляем концевые пробелы\n",
    "    aa=s.split(\"=\")                 #разбиваем по знаку =\n",
    "    var=aa[0]                       #слева имя словаря\n",
    "    aa=s.split('\"')                 #разбиваем по знаку '\"'\n",
    "    var=var+\".\"+aa[1]               #добавляем имя поля\n",
    "    aa=s.split(\":\")                 #разбиваем по знаку :\n",
    "    res.append(aa[1][:aa[1].find(\"}\")].strip()) #сохраняем то что справа од : до }\n",
    "    aa=script.split(var)            #разбиваем весь скрипт по имени переменной\n",
    "    for i in range(1, len(aa)-1):   #для всех подстрок от второй до предпоследней\n",
    "        res.append(aa[i][:aa[i].find(\";\")]) #сохраняем от начала до ;\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверяем как это работает\n",
    "rsp=hidemy()\n",
    "s=str(rsp.content.decode(rsp.encoding))\n",
    "bsObj=soup(s, \"lxml\")\n",
    "nums=findNums(bsObj)\n",
    "print(\"Получено \"+str(len(nums))+' \"чисел\"')\n",
    "for n in nums:\n",
    "    print(n)\n",
    "# Запустите этот тест несколько раз, чтобы посмотреть какие \"числа\" вываливаются \n",
    "# У меня получалось от 2 до 12 \"чисел\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим \"число\". Метод основан на моей беседе с **j0hnik** и сильно упрощён. Тем не менее те \"числа\" которые реально приходят, практически всегда считает правильно. Хотя 100% гарантии я дать не могу. Но 100% и не нужно. Если ошибёмся, попытку всегда можно повторить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalNun(s):\n",
    "    if s[0]!=\"+\":        #если \"число\" начинается не с + оно не содержит \n",
    "        a=s.split(\"[]\")  #групп в круглых скобках. Просто считаем число подстрок \"[]\"\n",
    "        return len(a)-1\n",
    "    else:\n",
    "        ss=\"\"\n",
    "        a=s.split(\")\")              # разбиваем на группы в круглых скобках\n",
    "        for i in range(0, len(a)):\n",
    "            if a[i]!=\"\":\n",
    "                s=a[i][a[i].rfind(\"(\")+1:]  #удаляем открывающиеся скобки\n",
    "                aa=s.split(\"[]\")            #разбиваем по подстрокам []\n",
    "                n=0\n",
    "                for q in aa:       #если длинна предшествующей строки больше 1\n",
    "                    if len(q)>1:   #то это 1\n",
    "                        n+=1\n",
    "                ss=ss+str(n) #интерпретируем полученное число, как символ числовой строки  \n",
    "        return int(ss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим как это работает\n",
    "rsp=hidemy()\n",
    "s=str(rsp.content.decode(rsp.encoding))\n",
    "bsObj=soup(s, \"lxml\")\n",
    "nums=findNums(bsObj)\n",
    "print(\"Получено \"+str(len(nums))+' \"чисел\"')\n",
    "print(evalNun(nums[0]))\n",
    "for i in range(1, len(nums)):\n",
    "    a=nums[i].split(\"=\")\n",
    "    print(evalNun(a[1]))\n",
    "print(\"====================================\")\n",
    "print(\"console.log(\"+nums[0]+\")\")\n",
    "for i in range(1, len(nums)):\n",
    "    a=nums[i].split(\"=\")\n",
    "    print(\"console.log(\"+a[1]+\")\")\n",
    "\n",
    "# То что распечатано после ============ скопируйте и вставьте в консоль браузера. \n",
    "# Убедитесь, что список чисел вверху совпадает с тем, что будет вычислено в консоли. \n",
    "# Сделайте этот тест несколько раз, чтобы убедиться, что если алгоритм и ошибается,\n",
    "# то весьма и весьма редко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем вычислить весь наш пример. \"числа\" мы сохраняли вместе с арифметическими операциями слева от знака =. Если посмотреть сохранённый файл, видно что в конце примера к результату надо прибавить **t.length**. Кэп подсказывает, что **t** это ни что иное, как строка **hidemy.name**. Делаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Код совершенно очевиден\n",
    "def evalAnsw(a):\n",
    "    num=evalNun(a[0])\n",
    "    for i in range(1, len(a)):\n",
    "        f=False\n",
    "        aa=a[i].split(\"=\")\n",
    "        nn=evalNun(aa[1])\n",
    "        if aa[0]==\"+\":\n",
    "            f=True\n",
    "            num+=nn\n",
    "        if aa[0]==\"-\":\n",
    "            f=True\n",
    "            num-=nn\n",
    "        if aa[0]==\"*\":\n",
    "            f=True\n",
    "            num*=nn\n",
    "        if f==False:\n",
    "            print(\"Unknown \"+aa[0])\n",
    "    return num+len(\"hidemy.name\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время собрать всё до кучи. И результат примера и форму и куку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собирает до кучи все параметры\n",
    "# Возвращает тройку url, cookies, param\n",
    "def getParams(rsp):\n",
    "    s=str(rsp.content.decode(rsp.encoding))\n",
    "    bsObj=soup(s, \"lxml\")\n",
    "    a=findNums(bsObj)\n",
    "    answ=evalAnsw(a)                               #решаем пример\n",
    "    form=bsObj.find(\"form\")                        #находим форму\n",
    "    url=\"https://hidemy.name\"+form.attrs[\"action\"] #добавляем её action к url\n",
    "    inps=form.find_all(\"input\")                    #находим все элементы input\n",
    "    params={}\n",
    "    for i in range(0, len(inps)):\n",
    "        val=inps[i].get(\"value\")                   #если value нет\n",
    "        if val==None:                              #подставляем туда пример\n",
    "            val=answ\n",
    "        params[inps[i].attrs[\"name\"]]=val           #собираем до кучи\n",
    "    cookeis=rsp.cookies.get_dict()\n",
    "    return (url, cookeis, params)\n",
    "\n",
    "getParams(rsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И вот он, критический момент !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Перепишем firefox поближе, чтобы было удобно экспериментировать\n",
    "firefox={\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",    \n",
    "\"Accept-Encoding\": \"gzip, deflate\", \n",
    "\"Accept-Language\": \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"Host\": \"hidemy.name\",\n",
    "\"Referer\": \"https://hidemy.name/en/proxy-list/\",    \n",
    "\"Upgrade-Insecure-Requests\": \"1\",    \n",
    "\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0\"   \n",
    "}\n",
    "\n",
    "rsp=hidemy()\n",
    "url, cookies, params=getParams(rsp)\n",
    "time.sleep(4)\n",
    "rsp1=rqs.get(url, headers=firefox, cookies=cookies, params=params)\n",
    "s=str(rsp1.content.decode(rsp1.encoding))\n",
    "print(rsp1.status_code)\n",
    "bsObj=soup(s, \"lxml\")\n",
    "title=bsObj.find(\"title\")\n",
    "print(str(title.contents[0]))\n",
    "file=open(\"hideme_proxy.html\", \"wt\")\n",
    "file.write(s)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили 200. Открываем сохраненный файл **hideme_proxy.html** и убеждаемся что пришел список прокси. Попробуем чуть-чуть \"пошевелить\" полученный результат. например поставить время ожидания 2 секунды или 30 секунд вместо 4. Или прибавить 1 к результату примера. Или изменить в заголовке поле Referer. Всё сразу перестаёт работать. Для гурманов-садомазо - попробуйте в поле заголовка **Accept-Encoding** добавить **br** (чтобы было **\"Accept-Encoding\": \"gzip, deflate, br\", **), получится смешно. Смех смехом, но между прочим firefox именно такое значение этого поля и прописывает. На результат я ффтыкал часа наверно три с гаком, не понимая кто меня дурит. Потом уже **br** от туда убрал. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем, можно ли получать прокси с установленной кукой, чтобы не ждать 4 секунды и не решать дурацких примеров по арифметике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://hidemy.name/ru/proxy-list/\"\n",
    "test=rqs.get(url, headers=firefox, cookies=cookies)\n",
    "s=str(test.content.decode(test.encoding))\n",
    "print(test.status_code)\n",
    "bsObj=soup(s, \"lxml\")\n",
    "title=bsObj.find(\"title\")\n",
    "print(str(title.contents[0]))\n",
    "file=open(\"hideme_test.html\", \"wt\")\n",
    "file.write(s)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не работает... Дело в том, что **rsp1** не простой ответ. На самом деле сервер сначала отвечает кодом **302**, означающим редирект. А уже после происходит сам редирект, с кодом **200**. Высокоуровневая библиотека **requests** этот момент скрывает. Но вот незадача, в ответе с кодом **302** приходит ещё одна кука. Естественно она у нас потеряна. Но она лежит в истории **rsp1**, откуда её можно достать. Сделаем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp302=rsp1.history[0]\n",
    "print(rsp302.status_code)\n",
    "cookies302=rsp302.cookies.get_dict()\n",
    "print(cookies302)\n",
    "cookies.update(cookies302)\n",
    "print(cookies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем снова тот же самый тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://hidemy.name/en/proxy-list/\"\n",
    "test=rqs.get(url, headers=firefox, cookies=cookies)\n",
    "s=str(test.content.decode(test.encoding))\n",
    "print(test.status_code)\n",
    "bsObj=soup(s, \"lxml\")\n",
    "title=bsObj.find(\"title\")\n",
    "print(str(title.contents[0]))\n",
    "file=open(\"hideme_test.html\", \"wt\")\n",
    "file.write(s)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё работает ! Отлично ! Теперь можно написать функцию, которая будет обращаться к сайту и при необходимости обновлять куки (они выдаются на сутки). Наша функция будет собирать прокси с указанного числа страничек (на каждой страничке 64 прокси), задавая их поиск параметрами поиска на самом сайте. Эти параметры следующие:\n",
    "1) Анонимность (высокая, средняя, низкая, нет)    \n",
    "2) Протокол (http, https, sock4, sock5)    \n",
    "3) Скорость не менее заданной (мс)    \n",
    "4) Страна    \n",
    "5) Порт    \n",
    "Чтобы не таскать всё это с собой, зададим параметрам значения по умолчанию:    \n",
    "Анонимность - высокая (5)        \n",
    "Протокол    - http (h)  \n",
    "Скорость    - 2000    \n",
    "Страна      - все     \n",
    "Порт        - все    \n",
    "Такие значения параметров удобны мне. У Вас могут быть другие предпочтения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookies={'__cfduid': 'dd1b26e2267ead10f15363f4066053f681517022909',\n",
    " 'cf_clearance': '511ce2492b2ae5a2b2cfeb2973de73ff2921fd63-1517022914-86400'}\n",
    "\n",
    "# Делает запрос с заданными параметрами\n",
    "# Если не получает ответа 200, пытается обновить куки\n",
    "# и повторить тот же запрос\n",
    "def reCookies(params):\n",
    "    global cookies\n",
    "    firefox={\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",    \n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"hidemy.name\",\n",
    "    \"Referer\": \"https://hidemy.name/en/proxy-list/\",    \n",
    "    \"Upgrade-Insecure-Requests\": \"1\",    \n",
    "    \"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0\"   \n",
    "    }\n",
    "    url=\"https://hidemy.name/en/proxy-list/\"  \n",
    "    if params!=\"\":\n",
    "        url=url+\"?\"+params\n",
    "    url=url+\"#list\"\n",
    "    for i in range(0, 10): #делаем 10 попыток\n",
    "        try:\n",
    "            rsp=rqs.get(url, headers=firefox, cookies=cookies)\n",
    "        except:\n",
    "            return None\n",
    "        if rsp.status_code==200:\n",
    "            return rsp\n",
    "        else:\n",
    "            url1, cookies, param1=getParams(rsp)  #получаем параметры обновления\n",
    "            time.sleep(4)    #ждем 4 секунды и обновляемся\n",
    "            try:\n",
    "                rsp=rqs.get(url1, headers=firefox, params=param1, cookies=cookies)\n",
    "            except:\n",
    "                return None\n",
    "            if rsp.status_code==200: #если получили 200, обновляем вторую куку\n",
    "                rsp302=rsp.history[0]\n",
    "                cookies302=rsp302.cookies.get_dict()\n",
    "                cookies.update(cookies302)      \n",
    "    return None #попытки закончились            \n",
    "   \n",
    "#https://hidemy.name/ru/proxy-list/?country=CAFRDEUS&maxtime=2000&type=h&anon=234&start=64#list\n",
    "#https://hidemy.name/ru/proxy-list/?country=CAFRDEUS&maxtime=2000&ports=8080&type=h&anon=234#list\n",
    "def scanHidemyPage(page, anon=\"4\", protocol=\"h\", speed=2000, country=\"\", ports=\"\"):\n",
    "    res=[]  #результат\n",
    "    #собираем параметры\n",
    "    #Увы, стандартно в виде словаря их передавать нельзя ибо на сайте \n",
    "    #проверяется порядок их следования. Поэтому их нужно явно собирать\n",
    "    #и передавать как часть строки запроса. \n",
    "    params=\"\"\n",
    "    if country!=\"\":\n",
    "        params=params+\"&country=\"+country\n",
    "    if speed!=0:\n",
    "        params=params+\"&maxtime=\"+str(speed)\n",
    "    if ports!=\"\":\n",
    "        params=params+\"&ports=\"+ports\n",
    "    if protocol!=\"\":\n",
    "        params=params+\"&type=\"+protocol\n",
    "    if anon!=\"\":\n",
    "        params=params+\"&anon=\"+anon\n",
    "    if page>1:\n",
    "        params=params+\"&start=\"+str((page-1)*64)\n",
    "        \n",
    "    params=params[1:]    \n",
    "    npages=-1              #число страниц в списке прокси\n",
    "    rsp=reCookies(params)  #получаем страницу сайта\n",
    "    if rsp==None:\n",
    "        return (npages, res)\n",
    "    try:\n",
    "        s=str(rsp.content.decode(rsp.encoding))\n",
    "        bsObj=soup(s, \"lxml\")\n",
    "        #Первым делом смотрим, сколько вообще есть страничек\n",
    "        pages=None\n",
    "        divs=bsObj.find_all(\"div\") #ищем все элементы div\n",
    "        for div in divs:\n",
    "            cc=div.attrs.get(\"class\")\n",
    "            if cc!=None:\n",
    "                if cc[0]==\"proxy__pagination\":\n",
    "                    pages=div                 #выбираем div с классом proxy__pagination\n",
    "                    break\n",
    "        if pages==None:\n",
    "            return (-1, res)\n",
    "        aa=pages.find_all(\"a\")                # ищем все теги a\n",
    "        #в тегах а ищем наибольшее число в содержимом\n",
    "        for a in aa:\n",
    "            ct=a.contents\n",
    "            if len(ct)>0:\n",
    "                nn=int(ct[0])\n",
    "                if nn>npages:\n",
    "                    npages=nn\n",
    "        if npages<page:                        #если страница больше чем число страниц\n",
    "            return (npages, res)               #читать разумеется нечего\n",
    "        #читаем прокси\n",
    "        trs=bsObj.find(\"tbody\").find_all(\"tr\") #получаем строки таблицы прокси\n",
    "        #Читаем из таблицы. Каждое поле имеет свою специфику, и код не комментирую \n",
    "        for tr in trs:\n",
    "            proxy={}                           #создаем новый прокси\n",
    "            tds=tr.find_all(\"td\")              #получаем td и читаем из них\n",
    "            proxy[\"ip\"]=str(tds[0].contents[0]).strip()\n",
    "            proxy[\"port\"]=str(tds[1].contents[0]).strip()\n",
    "            div=tds[2].contents[0]\n",
    "            proxy[\"country\"]=str(div.contents[1]).strip()\n",
    "            proxy[\"city\"]=\"\"\n",
    "            if len(div.contents)>2:\n",
    "                proxy[\"city\"]+\"|\"+str(div.contents[2]).strip()\n",
    "            p=tds[3].find(\"p\")\n",
    "            proxy[\"speed\"]=int(p.contents[0].split(\" \")[0])\n",
    "            pt=tds[4].contents[0].split(\",\")\n",
    "            for i in range(0, len(pt)):\n",
    "                pt[i]=pt[i].strip()\n",
    "            #print(\"speed\")    \n",
    "            proxy[\"protocol\"]=pt\n",
    "            anons=[\"No\",\"Low\", \"Medium\", \"High\"]\n",
    "            an=tds[5].contents[0].strip()\n",
    "            if an in anons:\n",
    "                proxy[\"anon\"]=anons.index(an)\n",
    "            else:\n",
    "                proxy[\"anon\"]=-1\n",
    "            ago=tds[6].contents[0].split(\" \")\n",
    "            proxy[\"ago\"]=int(ago[0])*60\n",
    "            if len(ago)>3:\n",
    "                proxy[\"ago\"]=60*(60*int(ago[0])+int(ago[2]))\n",
    "            res.append(proxy)        \n",
    "    except:\n",
    "        return (npages, res)#произошла какая-то неожиданность\n",
    "    return (npages, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверяем\n",
    "np, proxies=scanHidemyPage(1)\n",
    "print(np)\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим теперь как наши прокси работают. Как и с **[proxyrotator](https://www.proxyrotator.com/free-proxy-list/)**, используем для этого ресурс **[icanhazip](http://icanhazip.com/)**.   \n",
    "Внимание !!! Тест будет выполняться несколько минут !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "urlIpTest=\"http://icanhazip.com/\"\n",
    "s=\"\"\n",
    "try:\n",
    "    rsp=rqs.get(urlIpTest)\n",
    "    s=str(rsp.content.decode(rsp.encoding))\n",
    "except:\n",
    "    s=\"Ошибка\"\n",
    "print(\"Наш ip-адрес  ---> \"+s)\n",
    "\n",
    "timeout=5    #Зададим таймаут 5 секунд, чтобы не ждать слишком медленных прокси\n",
    "             #Вы эксперимента ради, можете задать его и больше. Меньше уже нежелательно\n",
    "             #Слишком много серверов отвалятся с ошибкой\n",
    "n=0\n",
    "nn=0\n",
    "speed=0     #Здесь Вы можете задать отбор прокси по скорости, от 1 до 5  \n",
    "for proxy in proxies:\n",
    "    n+=1\n",
    "    server=proxy[\"ip\"]+\":\"+str(proxy[\"port\"])\n",
    "    prox={\"http\" : server}\n",
    "    t1=time.time()\n",
    "    try:\n",
    "        rsp=rqs.get(urlIpTest, proxies=prox, timeout=timeout)\n",
    "        s=str(rsp.content.decode(rsp.encoding))\n",
    "        if(s[0]==\"<\"):\n",
    "            s=\"Забанен\"\n",
    "        else:\n",
    "            nn+=1\n",
    "    except:\n",
    "        s=\"Ошибка\"\n",
    "    t2=time.time()    \n",
    "    t=str(math.floor(t2-t1))\n",
    "    print(server+\" v=\"+str(proxy[\"speed\"])+\" => \"+s+\" t=\"+t)\n",
    "print(\"Выбрано прокси: \"+str(n))\n",
    "print(\"Без ошибок: \"+str(nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что ошибок сыплется много и прокси нужно проверять. Так же как и раньше, напишем функцию, пытающуюся найти не менее заданного количества прокси, работающих с нужным сайтом. Но поскольку теперь я пишу код не для себя, а предполагаю что им могут воспользоваться другие, я попытался сделать её более универсальной и полезной. Функция будет получать на вход проверочный url и функцию отбора. Функция отбора получает на вход объект bsObj и прокси с которого он был получен. Возвращать она должна **True** если прокси принят и **False** если отвергнут. Так же для связи с проверочным сайтом можно задавать заголовки, куки и параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Получает список прокси с hidemy.name\n",
    "#Проверяет каждый попыткой соединения c url\n",
    "#проверяя результат функцией отбора selFunc\n",
    "def getHidemyProxyList(nProxy, url, selFunc, hdrs={}, cooks={}, pars={}, tout=5, \n",
    "                anon=\"4\", protocol=\"h\", speed=2000, country=\"\", ports=\"\"):\n",
    "    nAccept=0                    #число принятых прокси\n",
    "    nth=0                        #число потоков\n",
    "    lock=threading.Lock()        #замок\n",
    "    done=threading.Event()       #событие окончания всех проверок\n",
    "\n",
    "    #Вложенная функция пытающаяся соединиться проверочным сервером\n",
    "    #и в случае получения какого-то ответа проверить, а тот ли это ответ что нужен\n",
    "    def test(p):                \n",
    "        nonlocal url, selFunc, nAccept, nth, lock, done \n",
    "        f=False\n",
    "        proxy={\"http\":p[\"ip\"]+\":\"+str(p[\"port\"])} \n",
    "        #Соединяемся и проверяем содержимое странички\n",
    "        #В случае успеха устанавливаем флаг f в True\n",
    "        try: \n",
    "            rsp=rqs.get(url, proxies=proxy, cookies=cooks, params=pars, timeout=tout)\n",
    "            s=str(rsp.content.decode(rsp.encoding))\n",
    "            bsObj=soup(s, \"lxml\") \n",
    "            if selFunc(p, bsObj):\n",
    "                f=True\n",
    "        except:\n",
    "            f=False\n",
    "        lock.acquire()     #Запираем замок, тут должно быть только однопоточное исполнение\n",
    "        if nth>0:          #Уменьшаем число потоков\n",
    "            nth-=1\n",
    "        if f:              #Если проверили успешно, увеличиваем число принятых прокси\n",
    "            nAccept+=1\n",
    "        if nth==0:         #Если число потоков равно нулю, запускаем событие\n",
    "            done.set()\n",
    "        lock.release()     #освобождаем замок\n",
    "    \n",
    "    page=1                 #страничка hidemy.name\n",
    "    npages=10              #число страничек\n",
    "    while page<=npages:    #пока есть доступные странички\n",
    "        #получаем страничку\n",
    "        npages, proxy=scanHidemyPage(page, anon=anon, protocol=protocol, \n",
    "                                   speed=speed, country=country, ports=ports)\n",
    "        if len(proxy)==0:\n",
    "            return page  #значит нечего больше делать\n",
    "        for p in proxy:  #для каждого прокси запускаем проверку в отдельном потоке\n",
    "            nth+=1     \n",
    "            th=threading.Thread(target=test, args=[p])\n",
    "            th.start()\n",
    "        done.wait()      #ждем окончания проверок\n",
    "        if nAccept>=nProxy:\n",
    "            return page  #отваливаемся, если собрали достаточно \n",
    "        page+=1          #переходим к следующей страничке\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку я работаю с сайтом избиркома, на нём и проверим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izbircomURL=\"http://www.vybory.izbirkom.ru/region/izbirkom?action=show&global=1&vrn=100100031793505&region=0&prver=0&pronetvd=null\"\n",
    "\n",
    "result=[]  #сюда будем складывать отобранные прокси\n",
    "def izbircomTest(proxy, bsObj):\n",
    "    try:\n",
    "        prx=proxy[\"ip\"]+\":\"+proxy[\"port\"]\n",
    "        if prx in result:\n",
    "            return False                     #старые прокси нам не нужны !\n",
    "        select=bsObj.find(\"select\")          #находим тег select\n",
    "        opt=select.find_all(\"option\")[12]    #смотрим что там в 12-й позиции\n",
    "        if str(opt.contents[0]).strip()==\"Республика Марий Эл\": \n",
    "            result.append(prx)               #если то что надо, отбираем прокси\n",
    "            return True                      #и сообщаем об этом\n",
    "        else:\n",
    "            return False                     #иначе не судьба\n",
    "    except:                                  #если произошла какая-то неожиданность\n",
    "        return False                         #очевидно нам этот прокси не годится\n",
    "    \n",
    "    \n",
    "#Проверяем\n",
    "pages=getHidemyProxyList(30, izbircomURL, izbircomTest)\n",
    "print(\"Просканировано \"+str(pages)+\" страничек\")\n",
    "print(\"Получено \"+str(len(result))+\" прокси\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом всё. Буду счастлив, если эта небольшая работа окажется кому-то полезной.  \n",
    "Ниже очищенный код библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rqs\n",
    "import time\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import threading \n",
    "\n",
    "def findNums(bsObj):\n",
    "    res=[]                                         #результат\n",
    "    script=str(bsObj.find(\"script\").contents[0])   #находим javascript\n",
    "    a=script.split(\"\\n\")                           #разбиваем по символам новой строки\n",
    "    mx=0                                 #максимум числа запятых в строке\n",
    "    idx=0                                #местоположение максимума\n",
    "    for i in range(0, len(a)):      #ищем строку с максимальным количеством запятых\n",
    "        aa=a[i].split(\",\")\n",
    "        if len(aa)>mx:\n",
    "            mx=len(aa)\n",
    "            idx=i\n",
    "    s=a[idx]                        #берём её часть справа от последней запятой\n",
    "    s=s[s.rfind(\",\")+1:].strip()    #и удаляем концевые пробелы\n",
    "    aa=s.split(\"=\")                 #разбиваем по знаку =\n",
    "    var=aa[0]                       #слева имя словаря\n",
    "    aa=s.split('\"')                 #разбиваем по знаку '\"'\n",
    "    var=var+\".\"+aa[1]               #добавляем имя поля\n",
    "    aa=s.split(\":\")                 #разбиваем по знаку :\n",
    "    res.append(aa[1][:aa[1].find(\"}\")].strip()) #сохраняем то что справа од : до }\n",
    "    aa=script.split(var)            #разбиваем весь скрипт по имени переменной\n",
    "    for i in range(1, len(aa)-1):   #для всех подстрок от второй до предпоследней\n",
    "        res.append(aa[i][:aa[i].find(\";\")]) #сохраняем от начала до ;\n",
    "    return res\n",
    "\n",
    "def evalNun(s):\n",
    "    if s[0]!=\"+\":        #если \"число\" начинается не с + оно не содержит \n",
    "        a=s.split(\"[]\")  #групп в круглых скобках. Просто считаем число подстрок \"[]\"\n",
    "        return len(a)-1\n",
    "    else:\n",
    "        ss=\"\"\n",
    "        a=s.split(\")\")              # разбиваем на группы в круглых скобках\n",
    "        for i in range(0, len(a)):\n",
    "            if a[i]!=\"\":\n",
    "                s=a[i][a[i].rfind(\"(\")+1:]  #удаляем открывающиеся скобки\n",
    "                aa=s.split(\"[]\")            #разбиваем по подстрокам []\n",
    "                n=0\n",
    "                for q in aa:       #если длинна предшествующей строки больше 1\n",
    "                    if len(q)>1:   #то это 1\n",
    "                        n+=1\n",
    "                ss=ss+str(n) #интерпретируем полученное число, как символ числовой строки  \n",
    "        return int(ss)        \n",
    "    \n",
    "# Код совершенно очевиден\n",
    "def evalAnsw(a):\n",
    "    num=evalNun(a[0])\n",
    "    for i in range(1, len(a)):\n",
    "        f=False\n",
    "        aa=a[i].split(\"=\")\n",
    "        nn=evalNun(aa[1])\n",
    "        if aa[0]==\"+\":\n",
    "            f=True\n",
    "            num+=nn\n",
    "        if aa[0]==\"-\":\n",
    "            f=True\n",
    "            num-=nn\n",
    "        if aa[0]==\"*\":\n",
    "            f=True\n",
    "            num*=nn\n",
    "        if f==False:\n",
    "            print(\"Unknown \"+aa[0])\n",
    "    return num+len(\"hidemy.name\")  \n",
    "\n",
    "# Собирает до кучи все параметры\n",
    "# Возвращает тройку url, cookies, param\n",
    "def getParams(rsp):\n",
    "    s=str(rsp.content.decode(rsp.encoding))\n",
    "    bsObj=soup(s, \"lxml\")\n",
    "    a=findNums(bsObj)\n",
    "    answ=evalAnsw(a)                               #решаем пример\n",
    "    form=bsObj.find(\"form\")                        #находим форму\n",
    "    url=\"https://hidemy.name\"+form.attrs[\"action\"] #добавляем её action к url\n",
    "    inps=form.find_all(\"input\")                    #находим все элементы input\n",
    "    params={}\n",
    "    for i in range(0, len(inps)):\n",
    "        val=inps[i].get(\"value\")                   #если value нет\n",
    "        if val==None:                              #подставляем туда пример\n",
    "            val=answ\n",
    "        params[inps[i].attrs[\"name\"]]=val           #собираем до кучи\n",
    "    cookeis=rsp.cookies.get_dict()\n",
    "    return (url, cookeis, params)\n",
    "\n",
    "cookies={'__cfduid': 'dd1b26e2267ead10f15363f4066053f681517022909',\n",
    " 'cf_clearance': '511ce2492b2ae5a2b2cfeb2973de73ff2921fd63-1517022914-86400'}\n",
    "\n",
    "# Делает запрос с заданными параметрами\n",
    "# Если не получает ответа 200, пытается обновить куки\n",
    "# и повторить тот же запрос\n",
    "def reCookies(params):\n",
    "    global cookies\n",
    "    firefox={\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",    \n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"hidemy.name\",\n",
    "    \"Referer\": \"https://hidemy.name/en/proxy-list/\",    \n",
    "    \"Upgrade-Insecure-Requests\": \"1\",    \n",
    "    \"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0\"   \n",
    "    }\n",
    "    url=\"https://hidemy.name/en/proxy-list/\"  \n",
    "    if params!=\"\":\n",
    "        url=url+\"?\"+params\n",
    "    url=url+\"#list\"\n",
    "    for i in range(0, 10): #делаем 10 попыток\n",
    "        try:\n",
    "            rsp=rqs.get(url, headers=firefox, cookies=cookies)\n",
    "        except:\n",
    "            return None\n",
    "        if rsp.status_code==200:\n",
    "            return rsp\n",
    "        else:\n",
    "            url1, cookies, param1=getParams(rsp)  #получаем параметры обновления\n",
    "            time.sleep(4)    #ждем 4 секунды и обновляемся\n",
    "            try:\n",
    "                rsp=rqs.get(url1, headers=firefox, params=param1, cookies=cookies)\n",
    "            except:\n",
    "                return None\n",
    "            if rsp.status_code==200: #если получили 200, обновляем вторую куку\n",
    "                rsp302=rsp.history[0]\n",
    "                cookies302=rsp302.cookies.get_dict()\n",
    "                cookies.update(cookies302)      \n",
    "    return None #попытки закончились            \n",
    "   \n",
    "#https://hidemy.name/ru/proxy-list/?country=CAFRDEUS&maxtime=2000&type=h&anon=234&start=64#list\n",
    "#https://hidemy.name/ru/proxy-list/?country=CAFRDEUS&maxtime=2000&ports=8080&type=h&anon=234#list\n",
    "def scanHidemyPage(page, anon=\"4\", protocol=\"h\", speed=2000, country=\"\", ports=\"\"):\n",
    "    res=[]  #результат\n",
    "    #собираем параметры\n",
    "    #Увы, стандартно в виде словаря их передавать нельзя ибо на сайте \n",
    "    #проверяется порядок их следования. Поэтому их нужно явно собирать\n",
    "    #и передавать как часть строки запроса. \n",
    "    params=\"\"\n",
    "    if country!=\"\":\n",
    "        params=params+\"&country=\"+country\n",
    "    if speed!=0:\n",
    "        params=params+\"&maxtime=\"+str(speed)\n",
    "    if ports!=\"\":\n",
    "        params=params+\"&ports=\"+ports\n",
    "    if protocol!=\"\":\n",
    "        params=params+\"&type=\"+protocol\n",
    "    if anon!=\"\":\n",
    "        params=params+\"&anon=\"+anon\n",
    "    if page>1:\n",
    "        params=params+\"&start=\"+str((page-1)*64)\n",
    "        \n",
    "    params=params[1:]    \n",
    "    npages=-1              #число страниц в списке прокси\n",
    "    rsp=reCookies(params)  #получаем страницу сайта\n",
    "    if rsp==None:\n",
    "        return (npages, res)\n",
    "    try:\n",
    "        s=str(rsp.content.decode(rsp.encoding))\n",
    "        bsObj=soup(s, \"lxml\")\n",
    "        #Первым делом смотрим, сколько вообще есть страничек\n",
    "        pages=None\n",
    "        divs=bsObj.find_all(\"div\") #ищем все элементы div\n",
    "        for div in divs:\n",
    "            cc=div.attrs.get(\"class\")\n",
    "            if cc!=None:\n",
    "                if cc[0]==\"proxy__pagination\":\n",
    "                    pages=div                 #выбираем div с классом proxy__pagination\n",
    "                    break\n",
    "        if pages==None:\n",
    "            return (-1, res)\n",
    "        aa=pages.find_all(\"a\")                # ищем все теги a\n",
    "        #в тегах а ищем наибольшее число в содержимом\n",
    "        for a in aa:\n",
    "            ct=a.contents\n",
    "            if len(ct)>0:\n",
    "                nn=int(ct[0])\n",
    "                if nn>npages:\n",
    "                    npages=nn\n",
    "        if npages<page:                        #если страница больше чем число страниц\n",
    "            return (npages, res)               #читать разумеется нечего\n",
    "        #читаем прокси\n",
    "        trs=bsObj.find(\"tbody\").find_all(\"tr\") #получаем строки таблицы прокси\n",
    "        #Читаем из таблицы. Каждое поле имеет свою специфику, и код не комментирую \n",
    "        for tr in trs:\n",
    "            proxy={}                           #создаем новый прокси\n",
    "            tds=tr.find_all(\"td\")              #получаем td и читаем из них\n",
    "            proxy[\"ip\"]=str(tds[0].contents[0]).strip()\n",
    "            proxy[\"port\"]=str(tds[1].contents[0]).strip()\n",
    "            div=tds[2].contents[0]\n",
    "            proxy[\"country\"]=str(div.contents[1]).strip()\n",
    "            proxy[\"city\"]=\"\"\n",
    "            if len(div.contents)>2:\n",
    "                proxy[\"city\"]+\"|\"+str(div.contents[2]).strip()\n",
    "            p=tds[3].find(\"p\")\n",
    "            proxy[\"speed\"]=int(p.contents[0].split(\" \")[0])\n",
    "            pt=tds[4].contents[0].split(\",\")\n",
    "            for i in range(0, len(pt)):\n",
    "                pt[i]=pt[i].strip()\n",
    "            #print(\"speed\")    \n",
    "            proxy[\"protocol\"]=pt\n",
    "            anons=[\"No\",\"Low\", \"Medium\", \"High\"]\n",
    "            an=tds[5].contents[0].strip()\n",
    "            if an in anons:\n",
    "                proxy[\"anon\"]=anons.index(an)\n",
    "            else:\n",
    "                proxy[\"anon\"]=-1\n",
    "            ago=tds[6].contents[0].split(\" \")\n",
    "            proxy[\"ago\"]=int(ago[0])*60\n",
    "            if len(ago)>3:\n",
    "                proxy[\"ago\"]=60*(60*int(ago[0])+int(ago[2]))\n",
    "            res.append(proxy)        \n",
    "    except:\n",
    "        return (npages, res)#произошла какая-то неожиданность\n",
    "    return (npages, res)\n",
    "\n",
    "#Получает список прокси с hidemy.name\n",
    "#Проверяет каждый попыткой соединения c url\n",
    "#проверяя результат функцией отбора selFunc\n",
    "def getHidemyProxyList(nProxy, url, selFunc, hdrs={}, cooks={}, pars={}, tout=5, \n",
    "                anon=\"4\", protocol=\"h\", speed=2000, country=\"\", ports=\"\"):\n",
    "    nAccept=0                    #число принятых прокси\n",
    "    nth=0                        #число потоков\n",
    "    lock=threading.Lock()        #замок\n",
    "    done=threading.Event()       #событие окончания всех проверок\n",
    "\n",
    "    #Вложенная функция пытающаяся соединиться проверочным сервером\n",
    "    #и в случае получения какого-то ответа проверить, а тот ли это ответ что нужен\n",
    "    def test(p):                \n",
    "        nonlocal url, selFunc, nAccept, nth, lock, done \n",
    "        f=False\n",
    "        proxy={\"http\":p[\"ip\"]+\":\"+str(p[\"port\"])} \n",
    "        #Соединяемся и проверяем содержимое странички\n",
    "        #В случае успеха устанавливаем флаг f в True\n",
    "        try: \n",
    "            rsp=rqs.get(url, proxies=proxy, cookies=cooks, params=pars, timeout=tout)\n",
    "            s=str(rsp.content.decode(rsp.encoding))\n",
    "            bsObj=soup(s, \"lxml\") \n",
    "            if selFunc(p, bsObj):\n",
    "                f=True\n",
    "        except:\n",
    "            f=False\n",
    "        lock.acquire()     #Запираем замок, тут должно быть только однопоточное исполнение\n",
    "        if nth>0:          #Уменьшаем число потоков\n",
    "            nth-=1\n",
    "        if f:              #Если проверили успешно, увеличиваем число принятых прокси\n",
    "            nAccept+=1\n",
    "        if nth==0:         #Если число потоков равно нулю, запускаем событие\n",
    "            done.set()\n",
    "        lock.release()     #освобождаем замок\n",
    "    \n",
    "    page=1                 #страничка hidemy.name\n",
    "    npages=10              #число страничек\n",
    "    while page<=npages:    #пока есть доступные странички\n",
    "        #получаем страничку\n",
    "        npages, proxy=scanHidemyPage(page, anon=anon, protocol=protocol, \n",
    "                                   speed=speed, country=country, ports=ports)\n",
    "        if len(proxy)==0:\n",
    "            return page  #значит нечего больше делать\n",
    "        for p in proxy:  #для каждого прокси запускаем проверку в отдельном потоке\n",
    "            nth+=1     \n",
    "            th=threading.Thread(target=test, args=[p])\n",
    "            th.start()\n",
    "        done.wait()      #ждем окончания проверок\n",
    "        if nAccept>=nProxy:\n",
    "            return page  #отваливаемся, если собрали достаточно \n",
    "        page+=1          #переходим к следующей страничке\n",
    "    return page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
